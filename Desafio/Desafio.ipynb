{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Geração de Resumo de notícias - Extração**\n",
        "\n",
        "\n",
        "1.   Definir fonte das notícias (NewsAPI);\n",
        "2.   Restringir área de atuação (Brazil - daily headlines ;\n",
        "3.   Definir os campos de interesse (Título=_title_/Conteúdo completo=_content_);\n"
      ],
      "metadata": {
        "id": "ZEWPF2kmfyML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5h6k-Mffe-x"
      },
      "outputs": [],
      "source": [
        "#!pip install newsapi-python - instala cliente do site\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#init atribui a pai key cedida pelo site a uma variável\n",
        "newsapi = NewsApiClient(api_key='a1b8895308db402fa4f3bdc5ba602871')\n",
        "\n",
        "#sources = newsapi.get_sources() gera uma variável com todas as fontes disponiveis (é bastabte coisa)\n",
        "\n",
        "#busca todos os artigos da fonte \"O Globo\", cujo assunto seja \"Science\"\n",
        "all_articles = newsapi.get_everything(sources='globo', q='science')\n",
        "\n",
        "#print(all_articles) para localizar os rotulos das informações necessárias\n",
        "\n",
        "# Converta os dados em um DataFrame, Salve em um  CSV para ir acompanhando\n",
        "df = pd.DataFrame(all_articles['articles'])\n",
        "df.to_csv('news_data.csv', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geração de resumo de notícias - Transformação**\n",
        "\n",
        "1.  Pré-processamento do texto:\n",
        "    \n",
        "    * Limpeza do texto (remover caracteres especiais, pontuações e formatações HTML que possam atrapalhar a IA;\n",
        "    * Tokenização (separar o texto em palavras ou frases significativas, os tokens);\n",
        "    * Remoção de Stopwords (remoão de palavrascomuns que não contribuem para o significado do texto)\n",
        "\n",
        "2. Geração de resumos\n",
        "\n",
        "    * Caçar uma ia que faça isso pra gente no hugging face;\n",
        "    * testada duas e a que obteve melhor resultados foi a _facebook/bart-large-cnn_ em detrimnenbto a _phpaiola/ptt5-base-summ-xlsum_\n",
        "\n",
        "3. Avaliação e aperfeiçoamento\n",
        "\n",
        "    * Resumos analisados pela mecânica de token do código ficaram bons, porém os títulos não, o ideal é manter os títulos originais.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8tZqon31p3UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install unidecode\n",
        "#!pip install nltk\n",
        "#!pip install transformers\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from unidecode import unidecode\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Inicializar o tokenizer e o modelo para sumarização\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Função para remoção de caracteres especiais\n",
        "def remove_special_characters(text):\n",
        "    text = unidecode(re.sub(r'[^\\w\\s]', ' ', text))\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Função para tokenização do texto\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def generate_summary(text):\n",
        "    inputs = tokenizer.encode(text, max_length=512, truncation=True, return_tensors='pt')\n",
        "    summary_ids = model.generate(inputs, max_length=150, num_beams=5, no_repeat_ngram_size=3, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Aplicar a função de remoção de caracteres especiais às colunas 'title' e 'content'\n",
        "#df['title'] = df['title'].apply(remove_special_characters)\n",
        "#df['content'] = df['content'].apply(remove_special_characters)\n",
        "\n",
        "# Aplicar a função de tokenização às colunas 'title' e 'content'\n",
        "#df['title_tokens'] = df['title'].apply(tokenize_text)\n",
        "#df['content_tokens'] = df['content'].apply(tokenize_text)\n",
        "\n",
        "# Aplicar a função de sumarização às colunas 'title_tokens' e 'content_tokens'\n",
        "df['title_summary'] = df['title_tokens'].apply(lambda x: generate_summary(' '.join(x)))\n",
        "df['content_summary'] = df['content_tokens'].apply(lambda x: generate_summary(' '.join(x)))\n",
        "\n",
        "# Salvar o DataFrame de volta em um novo CSV\n",
        "df.to_csv('dados_sumarizados.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "EX5JhHznHZ5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carregamento / Load **\n",
        "\n",
        "1. Destino dos resumos\n",
        "    * Para fins de aprendizado, vou gerar um CSV com o Título Original e a Matéria resumida, juntamente com o Link da noticia. Nesse aspecto, tenho de recurar algumas informações do inicio do trabalho;\n",
        "\n",
        "2. Inserção dos Resumos\n",
        "    * Se fosse num recurso externo, haveiriam mais preeocupações, mas inicialmente ficara apenas em CSV mesmo"
      ],
      "metadata": {
        "id": "KUO5lOlPTaKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar os arquivos\n",
        "df_sumarizado = pd.read_csv('dados_sumarizados 2 ET.csv')\n",
        "df_original = pd.read_csv('news_data E.csv')\n",
        "\n",
        "# Selecionar as colunas relevantes\n",
        "df_sumarizado = df_sumarizado[['title', 'content_summary']]\n",
        "df_original = df_original[['title', 'url']]\n",
        "\n",
        "# Realizar a junção dos dataframes\n",
        "df_final = pd.merge(df_sumarizado, df_original, on='title', how='inner')\n",
        "\n",
        "# Salvar o novo dataset\n",
        "df_final.to_csv('novo_dataset ETL.csv', index=False)"
      ],
      "metadata": {
        "id": "lm2RASBjUPaR"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}